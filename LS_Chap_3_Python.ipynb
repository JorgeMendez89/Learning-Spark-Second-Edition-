{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Spark - Chapter 3 (Python)\n",
    "## Apache Spark’s Structured APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame using SparkSession\n",
    "spark = (SparkSession\n",
    ".builder\n",
    ".appName(\"AuthorsAges\")\n",
    ".getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "data_df = (spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [name#637, age#638L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, age: bigint\n",
      "LogicalRDD [name#637, age#638L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [name#637, age#638L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[name#637,age#638L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Brooke| 20|\n",
      "| Denny| 31|\n",
      "| Jules| 30|\n",
      "|    TD| 35|\n",
      "|Brooke| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the same names together, aggregate their ages, and compute an average\n",
    "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['name], [unresolvedalias('name, None), avg('age) AS avg(age)#653]\n",
      "+- LogicalRDD [name#637, age#638L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, avg(age): double\n",
      "Aggregate [name#637], [name#637, avg(age#638L) AS avg(age)#653]\n",
      "+- LogicalRDD [name#637, age#638L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [name#637], [name#637, avg(age#638L) AS avg(age)#653]\n",
      "+- LogicalRDD [name#637, age#638L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[name#637], functions=[avg(age#638L)], output=[name#637, avg(age)#653])\n",
      "+- Exchange hashpartitioning(name#637, 200), ENSURE_REQUIREMENTS, [id=#487]\n",
      "   +- *(1) HashAggregate(keys=[name#637], functions=[partial_avg(age#638L)], output=[name#637, sum#658, count#659L])\n",
      "      +- *(1) Scan ExistingRDD[name#637,age#638L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Brooke|\n",
      "| Denny|\n",
      "| Jules|\n",
      "|    TD|\n",
      "|Brooke|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the results of the final execution\n",
    "data_df.select(\"name\").where(col(\"name\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataFrame API\n",
    "#### Schemas and Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"author\", StringType(), False),\n",
    "StructField(\"title\", StringType(), False),\n",
    "StructField(\"pages\", IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for our data using DDL\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING,`Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    "        [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "        [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "        [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,[\"twitter\", \"FB\"]],\n",
    "        [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "        [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [Id#666, First#667, Last#668, Url#669, Published#670, Hits#671, Campaigns#672], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Id: int, First: string, Last: string, Url: string, Published: string, Hits: int, Campaigns: array<string>\n",
      "LogicalRDD [Id#666, First#667, Last#668, Url#669, Published#670, Hits#671, Campaigns#672], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [Id#666, First#667, Last#668, Url#669, Published#670, Hits#671, Campaigns#672], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[Id#666,First#667,Last#668,Url#669,Published#670,Hits#671,Campaigns#672]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Id,IntegerType,true),StructField(First,StringType,true),StructField(Last,StringType,true),StructField(Url,StringType,true),StructField(Published,StringType,true),StructField(Hits,IntegerType,true),StructField(Campaigns,ArrayType(StringType,true),true)))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",[\"twitter\", \"LinkedIn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter', 'LinkedIn']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access using index for individual items\n",
    "blog_row[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire deparment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python, define a schema\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "StructField('UnitID', StringType(), True),\n",
    "StructField('IncidentNumber', IntegerType(), True),\n",
    "StructField('CallType', StringType(), True),\n",
    "StructField('CallDate', StringType(), True),\n",
    "StructField('WatchDate', StringType(), True),\n",
    "StructField('CallFinalDisposition', StringType(), True),\n",
    "StructField('AvailableDtTm', StringType(), True),\n",
    "StructField('Address', StringType(), True),\n",
    "StructField('City', StringType(), True),\n",
    "StructField('Zipcode', IntegerType(), True),\n",
    "StructField('Battalion', StringType(), True),\n",
    "StructField('StationArea', StringType(), True),\n",
    "StructField('Box', StringType(), True),\n",
    "StructField('OriginalPriority', StringType(), True),\n",
    "StructField('Priority', StringType(), True),\n",
    "StructField('FinalPriority', IntegerType(), True),\n",
    "StructField('ALSUnit', BooleanType(), True),\n",
    "StructField('CallTypeGroup', StringType(), True),\n",
    "StructField('NumAlarms', IntegerType(), True),\n",
    "StructField('UnitType', StringType(), True),\n",
    "StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "StructField('FirePreventionDistrict', StringType(), True),\n",
    "StructField('SupervisorDistrict', StringType(), True),\n",
    "StructField('Neighborhood', StringType(), True),\n",
    "StructField('Location', StringType(), True),\n",
    "StructField('RowID', StringType(), True),\n",
    "StructField('Delay', FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_fire_file = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation[CallNumber#722,UnitID#723,IncidentNumber#724,CallType#725,CallDate#726,WatchDate#727,CallFinalDisposition#728,AvailableDtTm#729,Address#730,City#731,Zipcode#732,Battalion#733,StationArea#734,Box#735,OriginalPriority#736,Priority#737,FinalPriority#738,ALSUnit#739,CallTypeGroup#740,NumAlarms#741,UnitType#742,UnitSequenceInCallDispatch#743,FirePreventionDistrict#744,SupervisorDistrict#745,... 4 more fields] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "CallNumber: int, UnitID: string, IncidentNumber: int, CallType: string, CallDate: string, WatchDate: string, CallFinalDisposition: string, AvailableDtTm: string, Address: string, City: string, Zipcode: int, Battalion: string, StationArea: string, Box: string, OriginalPriority: string, Priority: string, FinalPriority: int, ALSUnit: boolean, CallTypeGroup: string, NumAlarms: int, UnitType: string, UnitSequenceInCallDispatch: int, FirePreventionDistrict: string, SupervisorDistrict: string, ... 4 more fields\n",
      "Relation[CallNumber#722,UnitID#723,IncidentNumber#724,CallType#725,CallDate#726,WatchDate#727,CallFinalDisposition#728,AvailableDtTm#729,Address#730,City#731,Zipcode#732,Battalion#733,StationArea#734,Box#735,OriginalPriority#736,Priority#737,FinalPriority#738,ALSUnit#739,CallTypeGroup#740,NumAlarms#741,UnitType#742,UnitSequenceInCallDispatch#743,FirePreventionDistrict#744,SupervisorDistrict#745,... 4 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation[CallNumber#722,UnitID#723,IncidentNumber#724,CallType#725,CallDate#726,WatchDate#727,CallFinalDisposition#728,AvailableDtTm#729,Address#730,City#731,Zipcode#732,Battalion#733,StationArea#734,Box#735,OriginalPriority#736,Priority#737,FinalPriority#738,ALSUnit#739,CallTypeGroup#740,NumAlarms#741,UnitType#742,UnitSequenceInCallDispatch#743,FirePreventionDistrict#744,SupervisorDistrict#745,... 4 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [CallNumber#722,UnitID#723,IncidentNumber#724,CallType#725,CallDate#726,WatchDate#727,CallFinalDisposition#728,AvailableDtTm#729,Address#730,City#731,Zipcode#732,Battalion#733,StationArea#734,Box#735,OriginalPriority#736,Priority#737,FinalPriority#738,ALSUnit#739,CallTypeGroup#740,NumAlarms#741,UnitType#742,UnitSequenceInCallDispatch#743,FirePreventionDistrict#744,SupervisorDistrict#745,... 4 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/Learnin..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<CallNumber:int,UnitID:string,IncidentNumber:int,CallType:string,CallDate:string,WatchDate:...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a DataFrame as a Parquet file or SQL table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquetPath = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/JM Jupyter/parquepython1/\"\n",
    "#fire_df.write.format(\"parquet\").save(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquetTable = \"parquetablepython6\"\n",
    "#fire_df.write.format(\"parquet\").saveAsTable(parquetTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_fire_df = (fire_df.select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\").where(col(\"CallType\") != \"Medical Incident\"))\n",
    "\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|DistictCallTypes|\n",
      "+----------------+\n",
      "|              30|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python, return number of distinct types of calls using countDistinct()\n",
    "fire_df.select(\"CallType\").where(col(\"CallType\").isNotNull()).agg(countDistinct(\"CallType\").alias(\"DistictCallTypes\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#distinct call types in the data\n",
    "(fire_df\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .distinct()\n",
    " .show(10, truncate = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming, adding, and dropping columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(new_fire_df\n",
    ".select(\"ResponseDelayedinMins\")\n",
    ".where(col(\"ResponseDelayedinMins\") > 5)\n",
    ".show(5, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change date format\n",
    "fire_ts_df = (new_fire_df\n",
    ".withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"CallDate\")\n",
    ".withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"WatchDate\")\n",
    ".withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\"MM/dd/yyyy hh:mm:ss a\"))\n",
    ".drop(\"AvailableDtTm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    ".select(year('IncidentDate'))\n",
    ".distinct()\n",
    ".orderBy(year('IncidentDate'))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what were the most common types of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull())\n",
    ".groupBy(\"CallType\")\n",
    ".count()\n",
    ".orderBy(\"count\", ascending=False)\n",
    ".show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other common DataFrame operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Data‐\n",
    "Frame API provides descriptive statistical methods like min(), max(), sum(), and avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    ".select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"), \n",
    "        F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-End DataFrame Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  What were all the different types of fire calls in 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|CallType                                    |\n",
      "+--------------------------------------------+\n",
      "|Elevator / Escalator Rescue                 |\n",
      "|Marine Fire                                 |\n",
      "|Aircraft Emergency                          |\n",
      "|Confined Space / Structure Collapse         |\n",
      "|Administrative                              |\n",
      "|Alarms                                      |\n",
      "|Odor (Strange / Unknown)                    |\n",
      "|Citizen Assist / Service Call               |\n",
      "|HazMat                                      |\n",
      "|Watercraft in Distress                      |\n",
      "|Explosion                                   |\n",
      "|Oil Spill                                   |\n",
      "|Vehicle Fire                                |\n",
      "|Suspicious Package                          |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|\n",
      "|Other                                       |\n",
      "|Outside Fire                                |\n",
      "|Traffic Collision                           |\n",
      "|Assist Police                               |\n",
      "|Gas Leak (Natural and LP Gases)             |\n",
      "+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df.select(\"CallType\").where(col(\"CallType\").isNotNull()).distinct().show(truncate = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What months within the year 2018 saw the highest number of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|month(IncidentDate)|count|\n",
      "+-------------------+-----+\n",
      "|10                 |1068 |\n",
      "|5                  |1047 |\n",
      "|3                  |1029 |\n",
      "|8                  |1021 |\n",
      "|1                  |1007 |\n",
      "|7                  |974  |\n",
      "|6                  |974  |\n",
      "|9                  |951  |\n",
      "|4                  |947  |\n",
      "|2                  |919  |\n",
      "|11                 |199  |\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df.select(\"IncidentDate\")\n",
    " .where(year(col(\"IncidentDate\")) == \"2018\")\n",
    " .groupBy(month(col(\"IncidentDate\")))\n",
    " .count()\n",
    " .orderBy(\"count\", ascending=False)\n",
    " .show(n=12, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which neighborhood in San Francisco generated the most fire calls in 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|Neighborhood                  |count|\n",
      "+------------------------------+-----+\n",
      "|Tenderloin                    |7067 |\n",
      "|South of Market               |5323 |\n",
      "|Mission                       |4666 |\n",
      "|Financial District/South Beach|3907 |\n",
      "|Bayview Hunters Point         |2643 |\n",
      "+------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    " .where(col(\"City\") == 'San Francisco')\n",
    " .groupBy(col(\"Neighborhood\"))\n",
    " .count()\n",
    " .orderBy(\"count\", ascending = False)\n",
    " .show(n = 5, truncate = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which neighborhoods had the worst response times to fire calls in 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|        Neighborhood|ResponseDelayedinMins|\n",
      "+--------------------+---------------------+\n",
      "|     Pacific Heights|                 2.95|\n",
      "|Bayview Hunters P...|                  4.7|\n",
      "|          Tenderloin|            2.4333334|\n",
      "|      Bernal Heights|                  1.5|\n",
      "|    Western Addition|            3.4833333|\n",
      "|Financial Distric...|                 1.75|\n",
      "|Oceanview/Merced/...|            2.7166667|\n",
      "|          Tenderloin|            1.7833333|\n",
      "|           Japantown|            1.5166667|\n",
      "| Castro/Upper Market|            2.7666667|\n",
      "|             Mission|            2.1833334|\n",
      "|           Excelsior|                  2.5|\n",
      "|            Nob Hill|            2.4166667|\n",
      "|      Outer Richmond|                 4.95|\n",
      "|             Mission|            1.4166666|\n",
      "|             Mission|            2.5333333|\n",
      "|             Mission|            1.8833333|\n",
      "|  West of Twin Peaks|                 5.35|\n",
      "|      Inner Richmond|                  2.0|\n",
      "|      Inner Richmond|            1.8166667|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df.select(\"Neighborhood\",\"ResponseDelayedinMins\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------+\n",
      "|Neighborhood         |avgTimeDelay      |\n",
      "+---------------------+------------------+\n",
      "|Treasure Island      |5.47149999499321  |\n",
      "|Presidio             |4.9653753549934505|\n",
      "|Mission Bay          |4.530760579728938 |\n",
      "|McLaren Park         |4.309822855580256 |\n",
      "|None                 |4.307180858534226 |\n",
      "|Twin Peaks           |4.294008398269729 |\n",
      "|Golden Gate Park     |4.249903662329421 |\n",
      "|Lakeshore            |4.201812146300208 |\n",
      "|Bayview Hunters Point|4.150424644461803 |\n",
      "|Seacliff             |4.137820510795483 |\n",
      "+---------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df.select(\"Neighborhood\",\"ResponseDelayedinMins\")\n",
    " .where(col(\"ResponseDelayedinMins\").isNotNull())\n",
    " .groupBy(col(\"Neighborhood\"))\n",
    " .agg(avg(col(\"ResponseDelayedinMins\")).alias(\"avgTimeDelay\"))\n",
    " .orderBy(\"avgTimeDelay\", ascending = False)\n",
    " .show(n = 10, truncate = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which week in the year in 2018 had the most fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|weekofyear(IncidentDate)|count|\n",
      "+------------------------+-----+\n",
      "|22                      |259  |\n",
      "|40                      |255  |\n",
      "|43                      |250  |\n",
      "|25                      |249  |\n",
      "|1                       |246  |\n",
      "|44                      |244  |\n",
      "|32                      |243  |\n",
      "|13                      |243  |\n",
      "|11                      |240  |\n",
      "|5                       |236  |\n",
      "|18                      |236  |\n",
      "|23                      |235  |\n",
      "|2                       |234  |\n",
      "|31                      |234  |\n",
      "|42                      |234  |\n",
      "|19                      |233  |\n",
      "|34                      |232  |\n",
      "|10                      |232  |\n",
      "|8                       |232  |\n",
      "|28                      |231  |\n",
      "|21                      |231  |\n",
      "|16                      |228  |\n",
      "|7                       |228  |\n",
      "|9                       |228  |\n",
      "|38                      |226  |\n",
      "|6                       |225  |\n",
      "|33                      |225  |\n",
      "|20                      |225  |\n",
      "|14                      |225  |\n",
      "|39                      |224  |\n",
      "|3                       |224  |\n",
      "|26                      |223  |\n",
      "|37                      |223  |\n",
      "|27                      |223  |\n",
      "|29                      |223  |\n",
      "|15                      |222  |\n",
      "|12                      |221  |\n",
      "|35                      |221  |\n",
      "|41                      |220  |\n",
      "|36                      |203  |\n",
      "|17                      |203  |\n",
      "|30                      |203  |\n",
      "|4                       |202  |\n",
      "|24                      |198  |\n",
      "|45                      |64   |\n",
      "+------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df.select(\"IncidentDate\")\n",
    " .where(year(col(\"IncidentDate\")) == \"2018\")\n",
    " .groupBy(weekofyear(col(\"IncidentDate\")))\n",
    " .count()\n",
    " .orderBy(\"count\", ascending=False)\n",
    " .show(n=52, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Is there a correlation between neighborhood, zip code, and number of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Zipcode|count|\n",
      "+-------+-----+\n",
      "|94102  |21840|\n",
      "|94103  |20897|\n",
      "|94110  |14801|\n",
      "|94109  |14686|\n",
      "|94124  |9236 |\n",
      "|94112  |8421 |\n",
      "|94115  |7812 |\n",
      "|94107  |6941 |\n",
      "|94122  |6355 |\n",
      "|94133  |6246 |\n",
      "|94117  |5804 |\n",
      "|94114  |5175 |\n",
      "|94118  |5157 |\n",
      "|94134  |5009 |\n",
      "|94121  |4555 |\n",
      "|94132  |4321 |\n",
      "|94105  |4236 |\n",
      "|94108  |4084 |\n",
      "|94116  |3933 |\n",
      "|94123  |3719 |\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    " .where(col(\"Zipcode\").isNotNull())\n",
    " .groupBy(col(\"Zipcode\"))\n",
    " .count()\n",
    " .orderBy(\"count\", ascending = False)\n",
    " .show(n = 20, truncate = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How can we use Parquet files or SQL tables to store this data and read it back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquetPath = \"/fire_ts_df/\"\n",
    "# fire_ts_df.write.format(\"parquet\").save(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquetTable = \"fire_ts_df_table1\"\n",
    "# fire_ts_df.write.format(\"parquet\").saveAsTable(parquetTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typed Objects, Untyped Objects, and Generic Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row = Row(350, True, \"Learning Spark 2E\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning Spark 2E'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
