{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Spark - Chapter 3 (Scala)\n",
    "## Apache Spark’s Structured APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002778.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1620586059217)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.avg\r\n",
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6da896b3\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame using SparkSession\n",
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(\"AuthorsAges\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataDF: org.apache.spark.sql.DataFrame = [name: string, age: int]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame of names and ages\n",
    "val dataDF = spark.createDataFrame(Seq((\"Brooke\", 20), (\"Brooke\", 25),\n",
    "(\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35))).toDF(\"name\", \"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgDF: org.apache.spark.sql.DataFrame = [name: string, avg(age): double]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Group the same names together, aggregate their ages, and compute an average\n",
    "val avgDF = dataDF.groupBy(\"name\").agg(avg(\"age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Show the results of the final execution\n",
    "avgDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataFrame API\n",
    "#### Spark’s Basic Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nameTypes: org.apache.spark.sql.types.StringType.type = StringType\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nameTypes = StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firstName: org.apache.spark.sql.types.StringType.type = StringType\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstName = nameTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastName: org.apache.spark.sql.types.StringType.type = StringType\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lastName = nameTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schemas and Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(author,StringType,false), StructField(title,StringType,false), StructField(pages,IntegerType,false))\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Array(StructField(\"author\", StringType, false),\n",
    "StructField(\"title\", StringType, false),\n",
    "StructField(\"pages\", IntegerType, false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: String = author STRING, title STRING, pages INT\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsonFile: String = C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/chapter3/data/blogs.json\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get the path to the JSON file\n",
    "val jsonFile = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/chapter3/data/blogs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Id,IntegerType,false), StructField(First,StringType,false), StructField(Last,StringType,false), StructField(Url,StringType,false), StructField(Published,StringType,false), StructField(Hits,IntegerType,false), StructField(Campaigns,ArrayType(StringType,true),false))\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define our schema programmatically\n",
    "val schema = StructType(Array(StructField(\"Id\", IntegerType, false),\n",
    "StructField(\"First\", StringType, false),\n",
    "StructField(\"Last\", StringType, false),\n",
    "StructField(\"Url\", StringType, false),\n",
    "StructField(\"Published\", StringType, false),\n",
    "StructField(\"Hits\", IntegerType, false),\n",
    "StructField(\"Campaigns\", ArrayType(StringType), false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blogsDF: org.apache.spark.sql.DataFrame = [Id: int, First: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame by reading from the JSON file\n",
    "// with a predefined schema\n",
    "val blogsDF = spark.read.schema(schema).json(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|Id |First    |Last   |Url              |Published|Hits |Campaigns                   |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|1  |Jules    |Damji  |https://tinyurl.1|1/4/2016 |4535 |[twitter, LinkedIn]         |\n",
      "|2  |Brooke   |Wenig  |https://tinyurl.2|5/5/2018 |8908 |[twitter, LinkedIn]         |\n",
      "|3  |Denny    |Lee    |https://tinyurl.3|6/7/2019 |7659 |[web, twitter, FB, LinkedIn]|\n",
      "|4  |Tathagata|Das    |https://tinyurl.4|5/12/2018|10568|[twitter, FB]               |\n",
      "|5  |Matei    |Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB, LinkedIn]|\n",
      "|6  |Reynold  |Xin    |https://tinyurl.6|3/2/2015 |25568|[twitter, LinkedIn]         |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Show the DataFrame schema as output\n",
    "blogsDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(Id, First, Last, Url, Published, Hits, Campaigns)\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogsDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.Column = Id\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Access a particular column with col and it returns a Column type\n",
    "blogsDF.col(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Use an expression to compute a value\n",
    "blogsDF.select(expr(\"Hits * 2\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// or use col to compute value\n",
    "blogsDF.select(col(\"Hits\") * 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Use an expression to compute big hitters for blogs\n",
    "// This adds a new column, Big Hitters, based on the conditional expression\n",
    "blogsDF.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Concatenate three columns, create a new column, and show the\n",
    "// newly created concatenated column\n",
    "blogsDF.withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\")))).select(col(\"AuthorsId\")).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// These statements return the same value, showing that\n",
    "// expr is the same as a col method call\n",
    "blogsDF.select(expr(\"Hits\")).show(2)\n",
    "blogsDF.select(col(\"Hits\")).show(2)\n",
    "blogsDF.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Sort by column \"Id\" in descending order\n",
    "blogsDF.sort(col(\"Id\").desc).show()\n",
    "blogsDF.sort($\"Id\".desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "blogRow: org.apache.spark.sql.Row = [6,Reynold,Xin,https://tinyurl.6,255568,3/2/2015,[Ljava.lang.String;@6fbb2bf9]\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "// Create a Row\n",
    "val blogRow = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",Array(\"twitter\", \"LinkedIn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Any = Reynold\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Access using index for individual items\n",
    "blogRow(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       Author|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rows: Seq[(String, String)] = List((Matei Zaharia,CA), (Reynold Xin,CA))\r\n",
       "authorsDF: org.apache.spark.sql.DataFrame = [Author: string, State: string]\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rows = Seq((\"Matei Zaharia\", \"CA\"), (\"Reynold Xin\", \"CA\"))\n",
    "val authorsDF = rows.toDF(\"Author\", \"State\")\n",
    "authorsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire deparment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampleDF: org.apache.spark.sql.DataFrame = [CallNumber: string, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampleDF = spark.read.option(\"samplingRatio\", 0.001).option(\"header\", true)\n",
    ".csv(\"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema firecalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireSchema: org.apache.spark.sql.types.StructType = StructType(StructField(CallNumber,IntegerType,true), StructField(UnitID,StringType,true), StructField(IncidentNumber,IntegerType,true), StructField(CallType,StringType,true), StructField(CallDate,StringType,true), StructField(WatchDate,StringType,true), StructField(CallFinalDisposition,StringType,true), StructField(AvailableDtTm,StringType,true), StructField(Address,StringType,true), StructField(City,StringType,true), StructField(Zipcode,IntegerType,true), StructField(Battalion,StringType,true), StructField(StationArea,StringType,true), StructField(Box,StringType,true), StructField(OriginalPriority,StringType,true), StructField(Priority,StringType,true), StructField(FinalPriority,IntegerType,true), StructField(ALSUnit,BooleanType,true)...\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),StructField(\"UnitID\", StringType, true),\n",
    "StructField(\"IncidentNumber\", IntegerType, true),\n",
    "StructField(\"CallType\", StringType, true),\n",
    "StructField(\"CallDate\", StringType, true),\n",
    "StructField(\"WatchDate\", StringType, true),\n",
    "StructField(\"CallFinalDisposition\", StringType, true),\n",
    "StructField(\"AvailableDtTm\", StringType, true),\n",
    "StructField(\"Address\", StringType, true),\n",
    "StructField(\"City\", StringType, true),\n",
    "StructField(\"Zipcode\", IntegerType, true),\n",
    "StructField(\"Battalion\", StringType, true),\n",
    "StructField(\"StationArea\", StringType, true),\n",
    "StructField(\"Box\", StringType, true),\n",
    "StructField(\"OriginalPriority\", StringType, true),\n",
    "StructField(\"Priority\", StringType, true),\n",
    "StructField(\"FinalPriority\", IntegerType, true),\n",
    "StructField(\"ALSUnit\", BooleanType, true),\n",
    "StructField(\"CallTypeGroup\", StringType, true),\n",
    "StructField(\"NumAlarms\", IntegerType, true),\n",
    "StructField(\"UnitType\", StringType, true),\n",
    "StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "StructField(\"SupervisorDistrict\", StringType, true),\n",
    "StructField(\"Neighborhood\", StringType, true),\n",
    "StructField(\"Location\", StringType, true),\n",
    "StructField(\"RowID\", StringType, true),\n",
    "StructField(\"Delay\", FloatType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sfFireFile: String = C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\r\n",
       "fireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read the file using the CSV DataFrameReader\n",
    "val sfFireFile=\"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\"\n",
    "val fireDF = spark.read.schema(fireSchema)\n",
    ".option(\"header\", \"true\")\n",
    ".csv(sfFireFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n",
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._ \n",
    "import org.apache.spark.sql.functions._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: float (nullable = true)\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a DataFrame as a Parquet file or SQL table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "// to save as a Parquet file\n",
    "// val parquetPath = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/JM Jupyter/parquescala1/\"\n",
    "// fireDF.write.format(\"parquet\").save(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "// to save as a table\n",
    "// val parquetTable = \"parquetablascala1\" // name of the table\n",
    "// fireDF.write.format(\"parquet\").saveAsTable(parquetTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fewFireDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [IncidentNumber: int, AvailableDtTm: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fewFireDF = fireDF.select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\").where($\"CallType\" =!= \"Medical Incident\")\n",
    "fewFireDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|30               |\n",
      "+-----------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireDF\n",
    ".select(\"CallType\")\n",
    ".where(col(\"CallType\").isNotNull)\n",
    ".agg(countDistinct('CallType) as 'DistinctCallTypes)\n",
    ".show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//distinct call types in the data\n",
    "fireDF\n",
    ".select(\"CallType\")\n",
    ".where($\"CallType\".isNotNull)\n",
    ".distinct()\n",
    ".show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming, adding, and dropping columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newFireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "newFireDF\n",
    ".select(\"ResponseDelayedinMins\")\n",
    ".where($\"ResponseDelayedinMins\" > 5)\n",
    ".show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireTsDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Change date format\n",
    "val fireTsDF = newFireDF\n",
    ".withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"CallDate\")\n",
    ".withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    ".drop(\"WatchDate\")\n",
    ".withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\"MM/dd/yyyy hh:mm:ss a\"))\n",
    ".drop(\"AvailableDtTm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF\n",
    ".select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    ".show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF\n",
    ".select(year(col(\"IncidentDate\")))\n",
    ".distinct()\n",
    ".orderBy(year($\"IncidentDate\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what were the most common types of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF\n",
    ".where(col(\"CallType\").isNotNull)\n",
    ".groupBy(\"CallType\")\n",
    ".count()\n",
    ".orderBy(desc(\"count\"))\n",
    ".show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other common DataFrame operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Data‐\n",
    "Frame API provides descriptive statistical methods like min(), max(), sum(), and avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{functions=>F}\r\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{functions => F}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF\n",
    ".select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"),\n",
    "        F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-End DataFrame Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  What were all the different types of fire calls in 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|CallType                                    |\n",
      "+--------------------------------------------+\n",
      "|Elevator / Escalator Rescue                 |\n",
      "|Marine Fire                                 |\n",
      "|Aircraft Emergency                          |\n",
      "|Confined Space / Structure Collapse         |\n",
      "|Administrative                              |\n",
      "|Alarms                                      |\n",
      "|Odor (Strange / Unknown)                    |\n",
      "|Citizen Assist / Service Call               |\n",
      "|HazMat                                      |\n",
      "|Watercraft in Distress                      |\n",
      "|Explosion                                   |\n",
      "|Oil Spill                                   |\n",
      "|Vehicle Fire                                |\n",
      "|Suspicious Package                          |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|\n",
      "|Other                                       |\n",
      "|Outside Fire                                |\n",
      "|Traffic Collision                           |\n",
      "|Assist Police                               |\n",
      "|Gas Leak (Natural and LP Gases)             |\n",
      "+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF.select(\"CallType\").where(col(\"CallType\").isNotNull).distinct.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What months within the year 2018 saw the highest number of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|month(IncidentDate)|count|\n",
      "+-------------------+-----+\n",
      "|10                 |1068 |\n",
      "|5                  |1047 |\n",
      "|3                  |1029 |\n",
      "|8                  |1021 |\n",
      "|1                  |1007 |\n",
      "|6                  |974  |\n",
      "|7                  |974  |\n",
      "|9                  |951  |\n",
      "|4                  |947  |\n",
      "|2                  |919  |\n",
      "|11                 |199  |\n",
      "+-------------------+-----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF.where(year($\"IncidentDate\") === \"2018\")\n",
    ".groupBy(month($\"IncidentDate\"))\n",
    ".count()\n",
    ".orderBy(desc(\"count\"))\n",
    ".show(12, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which neighborhood in San Francisco generated the most fire calls in 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|Neighborhood                  |count|\n",
      "+------------------------------+-----+\n",
      "|Tenderloin                    |7067 |\n",
      "|South of Market               |5323 |\n",
      "|Mission                       |4666 |\n",
      "|Financial District/South Beach|3907 |\n",
      "|Bayview Hunters Point         |2643 |\n",
      "|Sunset/Parkside               |1900 |\n",
      "|Western Addition              |1826 |\n",
      "|Nob Hill                      |1660 |\n",
      "|Castro/Upper Market           |1364 |\n",
      "|Outer Richmond                |1319 |\n",
      "|Hayes Valley                  |1317 |\n",
      "|North Beach                   |1182 |\n",
      "|Pacific Heights               |1111 |\n",
      "|West of Twin Peaks            |1103 |\n",
      "|Excelsior                     |1071 |\n",
      "|Chinatown                     |1025 |\n",
      "|Potrero Hill                  |995  |\n",
      "|Marina                        |976  |\n",
      "|Bernal Heights                |839  |\n",
      "|Haight Ashbury                |834  |\n",
      "+------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF.select(\"Neighborhood\").where(col(\"City\") === \"San Francisco\").groupBy(col(\"Neighborhood\")).count().orderBy(desc(\"count\")).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which neighborhoods had the worst response times to fire calls in 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------+\n",
      "|Neighborhood         |avgTimeDelay      |\n",
      "+---------------------+------------------+\n",
      "|Treasure Island      |5.47149999499321  |\n",
      "|Presidio             |4.9653753549934505|\n",
      "|Mission Bay          |4.530760579728938 |\n",
      "|McLaren Park         |4.309822855580256 |\n",
      "|None                 |4.307180858534226 |\n",
      "|Twin Peaks           |4.294008398269729 |\n",
      "|Golden Gate Park     |4.249903662329421 |\n",
      "|Lakeshore            |4.201812146300208 |\n",
      "|Bayview Hunters Point|4.150424644461803 |\n",
      "|Seacliff             |4.137820510795483 |\n",
      "+---------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF.select(\"Neighborhood\",\"ResponseDelayedinMins\")\n",
    ".where(col(\"ResponseDelayedinMins\").isNotNull)\n",
    ".groupBy(col(\"Neighborhood\"))\n",
    ".agg(avg(col(\"ResponseDelayedinMins\")).alias(\"avgTimeDelay\"))\n",
    ".orderBy(desc(\"avgTimeDelay\"))\n",
    ".show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which week in the year in 2018 had the most fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|weekofyear(IncidentDate)|count|\n",
      "+------------------------+-----+\n",
      "|22                      |259  |\n",
      "|40                      |255  |\n",
      "|43                      |250  |\n",
      "|25                      |249  |\n",
      "|1                       |246  |\n",
      "|44                      |244  |\n",
      "|13                      |243  |\n",
      "|32                      |243  |\n",
      "|11                      |240  |\n",
      "|5                       |236  |\n",
      "|18                      |236  |\n",
      "|23                      |235  |\n",
      "+------------------------+-----+\n",
      "only showing top 12 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "fireTsDF\n",
    ".select(\"IncidentDate\")\n",
    ".where(year($\"IncidentDate\") === \"2018\")\n",
    ".groupBy(weekofyear($\"IncidentDate\"))\n",
    ".count()\n",
    ".orderBy(desc(\"count\")).show(12, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is there a correlation between neighborhood, zip code, and number of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Zipcode|count|\n",
      "+-------+-----+\n",
      "|94102  |21840|\n",
      "|94103  |20897|\n",
      "|94110  |14801|\n",
      "|94109  |14686|\n",
      "|94124  |9236 |\n",
      "|94112  |8421 |\n",
      "|94115  |7812 |\n",
      "|94107  |6941 |\n",
      "|94122  |6355 |\n",
      "|94133  |6246 |\n",
      "|94117  |5804 |\n",
      "|94114  |5175 |\n",
      "|94118  |5157 |\n",
      "|94134  |5009 |\n",
      "|94121  |4555 |\n",
      "+-------+-----+\n",
      "only showing top 15 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\r\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fireTsDF\n",
    ".where(col(\"Zipcode\")isNotNull)\n",
    ".groupBy(col(\"Zipcode\"))\n",
    ".count().orderBy(desc(\"count\"))\n",
    ".show(15, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How can we use Parquet files or SQL tables to store this data and read it back?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val parquetPath = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/JM Jupyter/fireTsDF1/\"\n",
    "fireTsDF.write.format(\"parquet\").save(parquetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val parquetTable = \"fireTsDF_table1\" // name of the table\n",
    "fireTsDF.write.format(\"parquet\").saveAsTable(parquetTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typed Objects, Untyped Objects, and Generic Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "row: org.apache.spark.sql.Row = [350,true,Learning Spark 2E,null]\r\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "val row = Row(350, true, \"Learning Spark 2E\", null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: Int = 350\r\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.getInt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: Boolean = true\r\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.getBoolean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res31: String = Learning Spark 2E\r\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.getString(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets\n",
    "### Scala: Case classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceIoTData\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DeviceIoTData (battery_level: Long, c02_level: Long, cca2: String, \n",
    "                          cca3: String, cn: String, device_id: Long,\n",
    "                          device_name: String, humidity: Long, ip: String, \n",
    "                          latitude: Double, lcd: String, longitude: Double, \n",
    "                          scale:String, temp: Long, timestamp: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = spark.read.json(\"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/iot-devices/iot_devices.json\").as[DeviceIoTData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|cn           |device_id|device_name          |humidity|ip           |latitude|lcd   |longitude|scale  |temp|timestamp    |\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "|8            |868      |US  |USA |United States|1        |meter-gauge-1xbYRYcj |51      |68.161.225.1 |38.0    |green |-97.0    |Celsius|34  |1458444054093|\n",
      "|7            |1473     |NO  |NOR |Norway       |2        |sensor-pad-2n2Pea    |70      |213.161.254.1|62.47   |red   |6.15     |Celsius|11  |1458444054119|\n",
      "|2            |1556     |IT  |ITA |Italy        |3        |device-mac-36TWSKiT  |44      |88.36.5.1    |42.83   |red   |12.83    |Celsius|19  |1458444054120|\n",
      "|6            |1080     |US  |USA |United States|4        |sensor-pad-4mzWkz    |32      |66.39.173.154|44.06   |yellow|-121.32  |Celsius|28  |1458444054121|\n",
      "|4            |931      |PH  |PHL |Philippines  |5        |therm-stick-5gimpUrBB|62      |203.82.41.9  |14.58   |green |120.97   |Celsius|25  |1458444054122|\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterTempDS: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterTempDS = ds.filter(d => {d.temp > 30 && d.humidity > 70})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterTempDS: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterTempDS = ds.where(col(\"temp\") > 30  && col(\"humidity\") > 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+-------------+---------+----------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|cn           |device_id|device_name           |humidity|ip             |latitude|lcd   |longitude|scale  |temp|timestamp    |\n",
      "+-------------+---------+----+----+-------------+---------+----------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|0            |1466     |US  |USA |United States|17       |meter-gauge-17zb8Fghhl|98      |161.188.212.254|39.95   |red   |-75.16   |Celsius|31  |1458444054129|\n",
      "|9            |986      |FR  |FRA |France       |48       |sensor-pad-48jt4eL    |97      |90.37.208.1    |43.88   |green |4.9      |Celsius|31  |1458444054151|\n",
      "|8            |1436     |US  |USA |United States|54       |sensor-pad-5410CWPrNb6|73      |204.15.64.249  |32.89   |red   |-117.13  |Celsius|34  |1458444054155|\n",
      "|4            |1090     |US  |USA |United States|63       |device-mac-63GL4xSaZbj|91      |66.198.198.1   |44.56   |yellow|-105.67  |Celsius|31  |1458444054162|\n",
      "|4            |1072     |PH  |PHL |Philippines  |81       |device-mac-81nsKomrRe |90      |222.127.71.1   |14.55   |yellow|121.04   |Celsius|31  |1458444054172|\n",
      "+-------------+---------+----+----+-------------+---------+----------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "filterTempDS.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceTempByCountry\r\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DeviceTempByCountry(temp: Long, device_name: String, device_id: Long,cca3: String)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val dsTemp = ds\n",
    ".where(col(\"temp\") > 25)\n",
    ".map(col(\"temp\"), col(\"device_name\"), col(\"device_id\"), col(\"cca3\"))\n",
    ".toDF(\"temp\", \"device_name\", \"device_id\", \"cca3\")\n",
    ".as[DeviceTempByCountry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsTemp: org.apache.spark.sql.Dataset[DeviceTempByCountry] = [temp: bigint, device_name: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsTemp = ds.select(\"temp\",\"device_name\", \"device_id\",\"cca3\")\n",
    ".where(col(\"temp\") > 25)\n",
    ".toDF(\"temp\", \"device_name\", \"device_id\", \"cca3\")\n",
    ".as[DeviceTempByCountry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+---------+----+\n",
      "|temp|device_name          |device_id|cca3|\n",
      "+----+---------------------+---------+----+\n",
      "|34  |meter-gauge-1xbYRYcj |1        |USA |\n",
      "|28  |sensor-pad-4mzWkz    |4        |USA |\n",
      "|27  |sensor-pad-6al7RTAobR|6        |USA |\n",
      "|27  |sensor-pad-8xUD6pzsQI|8        |JPN |\n",
      "|26  |sensor-pad-10BsywSYUF|10       |USA |\n",
      "+----+---------------------+---------+----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "dsTemp.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeviceTempByCountry(34,meter-gauge-1xbYRYcj,1,USA)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device: DeviceTempByCountry = DeviceTempByCountry(34,meter-gauge-1xbYRYcj,1,USA)\r\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val device = dsTemp.first()\n",
    "println(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------+---------+----+\n",
      "|temp|         device_name|device_id|device_id|cca3|\n",
      "+----+--------------------+---------+---------+----+\n",
      "|  34|meter-gauge-1xbYRYcj|        1|        1| USA|\n",
      "|  28|   sensor-pad-4mzWkz|        4|        4| USA|\n",
      "|  27|sensor-pad-6al7RT...|        6|        6| USA|\n",
      "|  27|sensor-pad-8xUD6p...|        8|        8| JPN|\n",
      "|  26|sensor-pad-10Bsyw...|       10|       10| USA|\n",
      "|  31|meter-gauge-17zb8...|       17|       17| USA|\n",
      "|  31|sensor-pad-18XULN9Xv|       18|       18| CHN|\n",
      "|  29|meter-gauge-19eg1...|       19|       19| USA|\n",
      "|  30|  device-mac-21sjz5h|       21|       21| AUT|\n",
      "|  28|sensor-pad-24Pytz...|       24|       24| CAN|\n",
      "|  27|therm-stick-25kK6...|       25|       25| USA|\n",
      "|  27|sensor-pad-34F1Ju...|       34|       34| RUS|\n",
      "|  34| sensor-pad-40NjeMqS|       40|       40| FRA|\n",
      "|  27| sensor-pad-448DeWGL|       44|       44| DEU|\n",
      "|  26|device-mac-45fN2C...|       45|       45| ITA|\n",
      "|  29|meter-gauge-47WsF9s8|       47|       47| UKR|\n",
      "|  31|  sensor-pad-48jt4eL|       48|       48| FRA|\n",
      "|  29|  sensor-pad-50g2ukc|       50|       50| DEU|\n",
      "|  34|sensor-pad-5410CW...|       54|       54| USA|\n",
      "|  30| sensor-pad-564BJDqo|       56|       56| THA|\n",
      "+----+--------------------+---------+---------+----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dsTemp2: org.apache.spark.sql.Dataset[DeviceTempByCountry] = [temp: bigint, device_name: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsTemp2 = ds\n",
    ".select($\"temp\", $\"device_name\", $\"device_id\", $\"device_id\", $\"cca3\")\n",
    ".where(\"temp > 25\")\n",
    ".as[DeviceTempByCountry]\n",
    "dsTemp2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Dataset Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect failing devices with battery levels below a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+\n",
      "|device_id|         device_name|battery_level|\n",
      "+---------+--------------------+-------------+\n",
      "|   185211|device-mac-185211...|            0|\n",
      "|   185423|meter-gauge-18542...|            0|\n",
      "|   185231|meter-gauge-18523...|            0|\n",
      "|   185202|sensor-pad-185202...|            0|\n",
      "|   185240|sensor-pad-185240...|            0|\n",
      "|   185139|device-mac-185139...|            0|\n",
      "|   185255|therm-stick-18525...|            0|\n",
      "|   185141|meter-gauge-18514...|            0|\n",
      "|   185272|sensor-pad-185272...|            0|\n",
      "|   185143|meter-gauge-18514...|            0|\n",
      "|   185283|device-mac-185283...|            0|\n",
      "|   185105|therm-stick-18510...|            0|\n",
      "|   185292|sensor-pad-185292...|            0|\n",
      "|   185078|sensor-pad-185078...|            0|\n",
      "|   185297|meter-gauge-18529...|            0|\n",
      "|   185157|device-mac-185157...|            0|\n",
      "|   185316|sensor-pad-185316...|            0|\n",
      "|   185160|sensor-pad-185160...|            0|\n",
      "|   185324|sensor-pad-185324...|            0|\n",
      "|   185188|sensor-pad-185188...|            0|\n",
      "+---------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.select(\"device_id\",\"device_name\",\"battery_level\")\n",
    ".where($\"battery_level\" < 8)\n",
    ".orderBy(\"battery_level\")\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify offending countries with high levels of CO2 emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|cn              |avg_CO2           |\n",
      "+----------------+------------------+\n",
      "|Gabon           |1523.0            |\n",
      "|Falkland Islands|1424.0            |\n",
      "|Monaco          |1421.5            |\n",
      "|Kosovo          |1389.0            |\n",
      "|San Marino      |1379.6666666666667|\n",
      "|Liberia         |1374.5            |\n",
      "|Syria           |1345.8            |\n",
      "|Mauritania      |1344.4285714285713|\n",
      "|Congo           |1333.375          |\n",
      "|Tonga           |1323.0            |\n",
      "|East Timor      |1310.0            |\n",
      "|Guinea          |1308.0            |\n",
      "|Botswana        |1302.6666666666667|\n",
      "|Haiti           |1291.3333333333333|\n",
      "|Laos            |1291.0            |\n",
      "|Maldives        |1284.7272727272727|\n",
      "|Sint Maarten    |1282.2857142857142|\n",
      "|Andorra         |1279.0            |\n",
      "|Lesotho         |1274.6            |\n",
      "|Mozambique      |1264.0            |\n",
      "+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.select(\"cn\",\"c02_level\")\n",
    ".where($\"c02_level\".isNotNull)\n",
    ".groupBy($\"cn\")\n",
    ".agg(avg($\"c02_level\").alias(\"avg_CO2\"))\n",
    ".orderBy(desc(\"avg_CO2\"))\n",
    ".show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the min and max values for temperature, battery level, CO2, and humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+-------+-------+------------+------------+\n",
      "|Min_Temp|Max_Temp|Min_Batt|Max_Batt|Min_CO2|Max_CO2|Min_humidity|Max_humidity|\n",
      "+--------+--------+--------+--------+-------+-------+------------+------------+\n",
      "|      10|      34|       0|       9|    800|   1599|          25|          99|\n",
      "+--------+--------+--------+--------+-------+-------+------------+------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.select(min(\"temp\").alias(\"Min_Temp\"), max(\"temp\").alias(\"Max_Temp\"),\n",
    "          min(\"battery_level\").alias(\"Min_Batt\"), max(\"battery_level\").alias(\"Max_Batt\"),\n",
    "          min(\"c02_level\").alias(\"Min_CO2\"), max(\"c02_level\").alias(\"Max_CO2\"),\n",
    "          min(\"humidity\").alias(\"Min_humidity\"), max(\"humidity\").alias(\"Max_humidity\")\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- battery_level: long (nullable = true)\n",
      " |-- c02_level: long (nullable = true)\n",
      " |-- cca2: string (nullable = true)\n",
      " |-- cca3: string (nullable = true)\n",
      " |-- cn: string (nullable = true)\n",
      " |-- device_id: long (nullable = true)\n",
      " |-- device_name: string (nullable = true)\n",
      " |-- humidity: long (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lcd: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- scale: string (nullable = true)\n",
      " |-- temp: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sort and group by average temperature, CO2, humidity, and country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------+-------------+\n",
      "|cn                    |avg(temp)|avg(humidity)|\n",
      "+----------------------+---------+-------------+\n",
      "|Monaco                |34.0     |91.0         |\n",
      "|Anguilla              |34.0     |83.0         |\n",
      "|British Virgin Islands|34.0     |81.0         |\n",
      "|Turkmenistan          |34.0     |80.0         |\n",
      "|Suriname              |34.0     |79.0         |\n",
      "|Gibraltar             |34.0     |78.0         |\n",
      "|Liechtenstein         |34.0     |76.0         |\n",
      "|Vanuatu               |33.5     |84.0         |\n",
      "|Cameroon              |33.0     |91.0         |\n",
      "|Fiji                  |33.0     |78.0         |\n",
      "+----------------------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.select(\"temp\", \"humidity\", \"cn\")\n",
    ".where($\"temp\" > 25 && $\"humidity\" > 75)\n",
    ".groupBy($\"cn\")\n",
    ".avg()\n",
    ".sort($\"avg(temp)\".desc, $\"avg(humidity)\".desc).alias(\"avg_humidity\")\n",
    ".show(10, false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
