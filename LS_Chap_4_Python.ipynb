{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Spark - Chapter 4 (Python)\n",
    "## Spark SQL and DataFrames: Introduction to Built-in Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"SparkSQLExampleApp\")\n",
    "         .master(\"local\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Query Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "csvFile = \"./departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you\n",
    "# may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"header\", \"true\")\n",
    ".load(csvFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### distance is greater than 1,000 miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011625|   -4|    4330|   HNL|        JFK|\n",
      "|1211625|  115|    4330|   HNL|        JFK|\n",
      "|1021625|  110|    4330|   HNL|        JFK|\n",
      "|1031625|   -1|    4330|   HNL|        JFK|\n",
      "|1041625|   -7|    4330|   HNL|        JFK|\n",
      "|1051625|   18|    4330|   HNL|        JFK|\n",
      "|1061625|    0|    4330|   HNL|        JFK|\n",
      "|1071625|    0|    4330|   HNL|        JFK|\n",
      "|1081625|   -5|    4330|   HNL|        JFK|\n",
      "|1091625|    0|    4330|   HNL|        JFK|\n",
      "|1101625|   -2|    4330|   HNL|        JFK|\n",
      "|1111625|  -10|    4330|   HNL|        JFK|\n",
      "|1121625|    6|    4330|   HNL|        JFK|\n",
      "|1131625|    0|    4330|   HNL|        JFK|\n",
      "|1141625|   -1|    4330|   HNL|        JFK|\n",
      "|1151625|   -9|    4330|   HNL|        JFK|\n",
      "|1161625|   -8|    4330|   HNL|        JFK|\n",
      "|1171625|    0|    4330|   HNL|        JFK|\n",
      "|1181625|   -8|    4330|   HNL|        JFK|\n",
      "|1191625|  -10|    4330|   HNL|        JFK|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark.sql(\"\"\"SELECT * \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE distance > 1000 ORDER BY distance DESC\"\"\")\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all flights between San Francisco (SFO) and Chicago (ORD) with at least a two-hour delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1021430|  132|    1604|   ORD|        SFO|\n",
      "|1021605|  140|    1604|   ORD|        SFO|\n",
      "|1021025|  148|    1604|   ORD|        SFO|\n",
      "|1022015|  122|    1604|   ORD|        SFO|\n",
      "|1052015|  138|    1604|   ORD|        SFO|\n",
      "|1061430|  183|    1604|   ORD|        SFO|\n",
      "|1061900|  155|    1604|   ORD|        SFO|\n",
      "|1061025|  144|    1604|   ORD|        SFO|\n",
      "|1301430|  228|    1604|   ORD|        SFO|\n",
      "|1301605|  187|    1604|   ORD|        SFO|\n",
      "|1011744|  154|    1604|   ORD|        SFO|\n",
      "|1011541|  123|    1604|   ORD|        SFO|\n",
      "|1011310|  130|    1604|   ORD|        SFO|\n",
      "|1011903|  158|    1604|   ORD|        SFO|\n",
      "|1020948|  342|    1604|   ORD|        SFO|\n",
      "|1031744|  167|    1604|   ORD|        SFO|\n",
      "|1031541|  278|    1604|   ORD|        SFO|\n",
      "|1041744|  247|    1604|   ORD|        SFO|\n",
      "|1041310|  159|    1604|   ORD|        SFO|\n",
      "|1041901|  179|    1604|   ORD|        SFO|\n",
      "|1051744|  140|    1604|   ORD|        SFO|\n",
      "|1071853|  207|    1604|   ORD|        SFO|\n",
      "|1082018|  231|    1604|   ORD|        SFO|\n",
      "|1081525|  372|    1604|   ORD|        SFO|\n",
      "|1091310|  247|    1604|   ORD|        SFO|\n",
      "|1102018|  155|    1604|   ORD|        SFO|\n",
      "|1131058|  125|    1604|   ORD|        SFO|\n",
      "|1021430|  145|    1604|   ORD|        SFO|\n",
      "|1021915|  145|    1604|   ORD|        SFO|\n",
      "|1041915|  130|    1604|   ORD|        SFO|\n",
      "|1051915|  385|    1604|   ORD|        SFO|\n",
      "|1060800|  184|    1604|   ORD|        SFO|\n",
      "|1061915|  162|    1604|   ORD|        SFO|\n",
      "|1271820|  352|    1604|   ORD|        SFO|\n",
      "|1301820|  171|    1604|   ORD|        SFO|\n",
      "|1011410|  124|    1604|   SFO|        ORD|\n",
      "|1022330|  326|    1604|   SFO|        ORD|\n",
      "|1021410|  190|    1604|   SFO|        ORD|\n",
      "|1101410|  184|    1604|   SFO|        ORD|\n",
      "|1190925|  297|    1604|   SFO|        ORD|\n",
      "|1241110|  139|    1604|   SFO|        ORD|\n",
      "|1301800|  167|    1604|   SFO|        ORD|\n",
      "|1011237|  122|    1604|   SFO|        ORD|\n",
      "|1032258|  163|    1604|   SFO|        ORD|\n",
      "|1031920|  193|    1604|   SFO|        ORD|\n",
      "|1031755|  396|    1604|   SFO|        ORD|\n",
      "|1071040|  279|    1604|   SFO|        ORD|\n",
      "|1161210|  225|    1604|   SFO|        ORD|\n",
      "|1221040|  215|    1604|   SFO|        ORD|\n",
      "|1261104|  258|    1604|   SFO|        ORD|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay >= 120 \n",
    "AND \n",
    "(origin == 'SFO' AND destination = 'ORD' \n",
    "OR origin == 'ORD' AND destination = 'SFO')\"\"\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### label all US flights, regardless of origin and destination,with an indication of the delays they experienced: Very Long Delays (> 6 hours), Long Delays (2–6 hours), etc. We’ll add these human-readable labels in a new column called Flight_Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "CASE\n",
    "WHEN delay > 360 THEN 'Very Long Delays'\n",
    "WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "WHEN delay = 0 THEN 'No Delays'\n",
    "ELSE 'Early'\n",
    "END AS Flight_Delays\n",
    "FROM us_delay_flights_tbl\n",
    "ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### distance is greater than 1,000 miles in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011625|   -4|    4330|   HNL|        JFK|\n",
      "|1211625|  115|    4330|   HNL|        JFK|\n",
      "|1021625|  110|    4330|   HNL|        JFK|\n",
      "|1031625|   -1|    4330|   HNL|        JFK|\n",
      "|1041625|   -7|    4330|   HNL|        JFK|\n",
      "|1051625|   18|    4330|   HNL|        JFK|\n",
      "|1061625|    0|    4330|   HNL|        JFK|\n",
      "|1071625|    0|    4330|   HNL|        JFK|\n",
      "|1081625|   -5|    4330|   HNL|        JFK|\n",
      "|1091625|    0|    4330|   HNL|        JFK|\n",
      "|1101625|   -2|    4330|   HNL|        JFK|\n",
      "|1111625|  -10|    4330|   HNL|        JFK|\n",
      "|1121625|    6|    4330|   HNL|        JFK|\n",
      "|1131625|    0|    4330|   HNL|        JFK|\n",
      "|1141625|   -1|    4330|   HNL|        JFK|\n",
      "|1151625|   -9|    4330|   HNL|        JFK|\n",
      "|1161625|   -8|    4330|   HNL|        JFK|\n",
      "|1171625|    0|    4330|   HNL|        JFK|\n",
      "|1181625|   -8|    4330|   HNL|        JFK|\n",
      "|1191625|  -10|    4330|   HNL|        JFK|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col('distance')> 1000).orderBy(desc('distance')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    ".where(\"distance > 1000\")\n",
    ".orderBy(\"distance\", ascending=False).show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SQL Databases and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS learn_spark_db1\")\n",
    "spark.sql(\"USE learn_spark_db1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS managed_us_delay_flights_tbl2\n",
    "(date STRING, delay INT,distance INT, origin STRING, destination STRING)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(csvFile, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl_4(date STRING, delay INT, distance INT, origin STRING, destination STRING) \n",
    "USING csv OPTIONS (PATH'./departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(flights_df\n",
    ".write\n",
    ".option(\"path\", \"./us_flights_delay\")\n",
    ".saveAsTable(\"us_delay_flights_tbl_6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    ".where(\"distance > 1000\")\n",
    ".orderBy(\"distance\", ascending=False).show(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view \n",
    "AS SELECT date, delay, origin, destination \n",
    "from us_delay_flights_tbl WHERE origin = 'SFO';\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sources for DataFrames and SQL Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\"\n",
    "# Use Parquet\n",
    "df = (spark.read.format(\"parquet\").load(file))\n",
    "# Use Parquet; you can omit format(\"parquet\") if you wish as it's the default\n",
    "df2 = (spark.read.load(file))\n",
    "# Use CSV\n",
    "df3 = (spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"mode\", \"PERMISSIVE\")\n",
    "       .load(\"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"))\n",
    "# Use JSON\n",
    "df4 = (spark.read.format(\"json\").load(\"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Parquet files into a Spark SQL table\n",
    "(spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "USING parquet\n",
    "OPTIONS (path \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\" )\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"parquet\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"compression\", \"snappy\")\n",
    ".save(\"./df_parquet_flights\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"json\")\n",
    ".mode(\"overwrite\")\n",
    "#.option(\"compression\", \"snappy\")\n",
    ".save(\"./df_json_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing csv\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"./df_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME               |ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------------------+-------------------+-----+\n",
      "|United States                   |Romania            |1    |\n",
      "|United States                   |Ireland            |264  |\n",
      "|United States                   |India              |69   |\n",
      "|Egypt                           |United States      |24   |\n",
      "|Equatorial Guinea               |United States      |1    |\n",
      "|United States                   |Singapore          |25   |\n",
      "|United States                   |Grenada            |54   |\n",
      "|Costa Rica                      |United States      |477  |\n",
      "|Senegal                         |United States      |29   |\n",
      "|United States                   |Marshall Islands   |44   |\n",
      "|Guyana                          |United States      |17   |\n",
      "|United States                   |Sint Maarten       |53   |\n",
      "|Malta                           |United States      |1    |\n",
      "|Bolivia                         |United States      |46   |\n",
      "|Anguilla                        |United States      |21   |\n",
      "|Turks and Caicos Islands        |United States      |136  |\n",
      "|United States                   |Afghanistan        |2    |\n",
      "|Saint Vincent and the Grenadines|United States      |1    |\n",
      "|Italy                           |United States      |390  |\n",
      "|United States                   |Russia             |156  |\n",
      "+--------------------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark.read.format(\"avro\")\n",
    ".load(\"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW episode_tbl\n",
    "USING avro\n",
    "OPTIONS (\n",
    "path \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "df = spark.read.format(\"orc\").option(\"path\", file).load()\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"orc\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"compression\", \"snappy\")\n",
    ".save(\"./flights_orc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/JM Jupyter/cctvVideos/train_images/\"\n",
    "images_df = spark.read.format(\"image\").load(image_dir)\n",
    "images_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+----+-----+\n",
      "|height|width|nChannels|mode|label|\n",
      "+------+-----+---------+----+-----+\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |1    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "+------+-----+---------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_df.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "|                path|   modificationTime|length|             content|label|\n",
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 55037|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54634|[FF D8 FF E0 00 1...|    1|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54624|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54505|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54475|[FF D8 FF E0 00 1...|    0|\n",
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"./cctvVideos/train_images/\"\n",
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    ".option(\"pathGlobFilter\", \"*.jpg\")\n",
    ".load(path))\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 55037|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54634|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54624|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54505|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54475|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    ".option(\"pathGlobFilter\", \"*.jpg\")\n",
    ".option(\"recursiveFileLookup\", \"true\")\n",
    ".load(path))\n",
    "binary_files_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
