{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002778.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1620723792493)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.avg\r\n",
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@591f2d12\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame using SparkSession\n",
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(\"cap_3_extra\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mnmFile: String = C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/mnm_dataset.csv\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get the M&M data set filename\n",
    "val mnmFile = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/mnm_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mnmDF: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read the file into a Spark DataFrame\n",
    "val mnmDF = spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(mnmFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuando se define un schema al definir un campo por ejemplo StructField('Delay', FloatType(), True) ¿qué significa el último parámetro Boolean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que los valores que toma pueden ser nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset vs DataFrame (Scala). ¿En qué se diferencian a nivel de código?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las apis estan unificadas y se pueden utilizar los mismo codigos. Sin embargo par ala estructura, para datasets puede ser necesario hacer case para poder definirlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizando el mismo ejemplo utilizado en el capítulo para guardar en parquet y guardar los datos en los formatos:\n",
    "\n",
    "i. JSON\n",
    "\n",
    "ii. CSV (dándole otro nombre para evitar sobrescribir el fichero origen)\n",
    "\n",
    "iii. AVRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "//JSON\n",
    "mnmDF.write.format(\"json\")\n",
    ".mode(\"overwrite\")\n",
    "//.option(\"compression\", \"snappy\")\n",
    ".save(\"./mnmJson2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// CSV\n",
    "mnmDF.write.\n",
    "format(\"csv\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"./mnm_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// AVRO\n",
    "mnmDF.write\n",
    ".format(\"avro\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"./mnmDF_avro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
