{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Spark - Chapter 4 (Python)\n",
    "## Spark SQL and DataFrames: Introduction to Built-in Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002778.bosonit.local:4042\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1620586875147)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7fd1ffd4\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(\"SparkSQLExampleApp\")\n",
    ".master(\"local\")\n",
    ".config(\"spark.sql.warehouse.dir\", \"C:/Users/jorgedario.mendez/OneDrive%20-%20Bosonit/Documentos/3.%20Libro/LearningSparkV2-master/JM%20Jupyter/spark-warehouse\")\n",
    ".enableHiveSupport()\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "import spark.sql\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import spark.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Query Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvFile: String = ./departuredelays.csv\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Path to data set\n",
    "val csvFile = \"./departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read and create a temporary view\n",
    "// Infer schema (note that for larger files you may want to specify the schema)\n",
    "val df = spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"header\", \"true\")\n",
    ".load(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "|1080600|    0|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation[date#16,delay#17,distance#18,origin#19,destination#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "date: int, delay: int, distance: int, origin: string, destination: string\n",
      "Relation[date#16,delay#17,distance#18,origin#19,destination#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation[date#16,delay#17,distance#18,origin#19,destination#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [date#16,delay#17,distance#18,origin#19,destination#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/Learnin..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:int,delay:int,distance:int,origin:string,destination:string>\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a temporary view\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### distance is greater than 1,000 miles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1012355|    0|    1586|   ABQ|        JFK|\n",
      "|1022355|  158|    1586|   ABQ|        JFK|\n",
      "|1032355|    0|    1586|   ABQ|        JFK|\n",
      "|1042355|    0|    1586|   ABQ|        JFK|\n",
      "|1052355|    0|    1586|   ABQ|        JFK|\n",
      "|1062355|    0|    1586|   ABQ|        JFK|\n",
      "|1072359|   14|    1586|   ABQ|        JFK|\n",
      "|1082358|   -4|    1586|   ABQ|        JFK|\n",
      "|1092358|   20|    1586|   ABQ|        JFK|\n",
      "|1102358|   -2|    1586|   ABQ|        JFK|\n",
      "|1112358|  116|    1586|   ABQ|        JFK|\n",
      "|1122358|    0|    1586|   ABQ|        JFK|\n",
      "|1132358|  -10|    1586|   ABQ|        JFK|\n",
      "|1142359|  -17|    1586|   ABQ|        JFK|\n",
      "|1152358|  135|    1586|   ABQ|        JFK|\n",
      "|1162358|  -13|    1586|   ABQ|        JFK|\n",
      "|1172358|  -10|    1586|   ABQ|        JFK|\n",
      "|1182358|  -17|    1586|   ABQ|        JFK|\n",
      "|1192358|  -18|    1586|   ABQ|        JFK|\n",
      "|1202358|   -9|    1586|   ABQ|        JFK|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * \n",
    "FROM us_delay_flights_tbl\n",
    "WHERE distance > 1000\"\"\")\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all flights between San Francisco (SFO) and Chicago (ORD) with at least a two-hour delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1021430|  132|    1604|   ORD|        SFO|\n",
      "|1021605|  140|    1604|   ORD|        SFO|\n",
      "|1021025|  148|    1604|   ORD|        SFO|\n",
      "|1022015|  122|    1604|   ORD|        SFO|\n",
      "|1052015|  138|    1604|   ORD|        SFO|\n",
      "|1061430|  183|    1604|   ORD|        SFO|\n",
      "|1061900|  155|    1604|   ORD|        SFO|\n",
      "|1061025|  144|    1604|   ORD|        SFO|\n",
      "|1301430|  228|    1604|   ORD|        SFO|\n",
      "|1301605|  187|    1604|   ORD|        SFO|\n",
      "|1011744|  154|    1604|   ORD|        SFO|\n",
      "|1011541|  123|    1604|   ORD|        SFO|\n",
      "|1011310|  130|    1604|   ORD|        SFO|\n",
      "|1011903|  158|    1604|   ORD|        SFO|\n",
      "|1020948|  342|    1604|   ORD|        SFO|\n",
      "|1031744|  167|    1604|   ORD|        SFO|\n",
      "|1031541|  278|    1604|   ORD|        SFO|\n",
      "|1041744|  247|    1604|   ORD|        SFO|\n",
      "|1041310|  159|    1604|   ORD|        SFO|\n",
      "|1041901|  179|    1604|   ORD|        SFO|\n",
      "|1051744|  140|    1604|   ORD|        SFO|\n",
      "|1071853|  207|    1604|   ORD|        SFO|\n",
      "|1082018|  231|    1604|   ORD|        SFO|\n",
      "|1081525|  372|    1604|   ORD|        SFO|\n",
      "|1091310|  247|    1604|   ORD|        SFO|\n",
      "|1102018|  155|    1604|   ORD|        SFO|\n",
      "|1131058|  125|    1604|   ORD|        SFO|\n",
      "|1021430|  145|    1604|   ORD|        SFO|\n",
      "|1021915|  145|    1604|   ORD|        SFO|\n",
      "|1041915|  130|    1604|   ORD|        SFO|\n",
      "|1051915|  385|    1604|   ORD|        SFO|\n",
      "|1060800|  184|    1604|   ORD|        SFO|\n",
      "|1061915|  162|    1604|   ORD|        SFO|\n",
      "|1271820|  352|    1604|   ORD|        SFO|\n",
      "|1301820|  171|    1604|   ORD|        SFO|\n",
      "|1011410|  124|    1604|   SFO|        ORD|\n",
      "|1022330|  326|    1604|   SFO|        ORD|\n",
      "|1021410|  190|    1604|   SFO|        ORD|\n",
      "|1101410|  184|    1604|   SFO|        ORD|\n",
      "|1190925|  297|    1604|   SFO|        ORD|\n",
      "|1241110|  139|    1604|   SFO|        ORD|\n",
      "|1301800|  167|    1604|   SFO|        ORD|\n",
      "|1011237|  122|    1604|   SFO|        ORD|\n",
      "|1032258|  163|    1604|   SFO|        ORD|\n",
      "|1031920|  193|    1604|   SFO|        ORD|\n",
      "|1031755|  396|    1604|   SFO|        ORD|\n",
      "|1071040|  279|    1604|   SFO|        ORD|\n",
      "|1161210|  225|    1604|   SFO|        ORD|\n",
      "|1221040|  215|    1604|   SFO|        ORD|\n",
      "|1261104|  258|    1604|   SFO|        ORD|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 50 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 \n",
    "AND \n",
    "(origin == 'SFO' AND destination = 'ORD' \n",
    "OR origin == 'ORD' AND destination = 'SFO')\"\"\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### label all US flights, regardless of origin and destination,with an indication of the delays they experienced: Very Long Delays (> 6 hours), Long Delays (2–6 hours), etc. We’ll add these human-readable labels in a new column called Flight_Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|   Long Delay|\n",
      "|  305|   ABE|        ATL|   Long Delay|\n",
      "|  275|   ABE|        ATL|   Long Delay|\n",
      "|  257|   ABE|        ATL|   Long Delay|\n",
      "|  247|   ABE|        ATL|   Long Delay|\n",
      "|  247|   ABE|        DTW|   Long Delay|\n",
      "|  219|   ABE|        ORD|   Long Delay|\n",
      "|  211|   ABE|        ATL|   Long Delay|\n",
      "|  197|   ABE|        DTW|   Long Delay|\n",
      "|  192|   ABE|        ORD|   Long Delay|\n",
      "|  180|   ABE|        ATL|   Long Delay|\n",
      "|  173|   ABE|        DTW|   Long Delay|\n",
      "|  165|   ABE|        ATL|   Long Delay|\n",
      "|  159|   ABE|        ATL|   Long Delay|\n",
      "|  159|   ABE|        ORD|   Long Delay|\n",
      "|  158|   ABE|        ATL|   Long Delay|\n",
      "|  151|   ABE|        DTW|   Long Delay|\n",
      "|  127|   ABE|        ATL|   Long Delay|\n",
      "|  121|   ABE|        DTW|   Long Delay|\n",
      "|  118|   ABE|        DTW|  Short Delay|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "CASE \n",
    "WHEN delay > 360 THEN 'Very Long Delay' \n",
    "WHEN delay > 120 THEN 'Long Delay' \n",
    "WHEN delay > 60 THEN 'Short Delay' \n",
    "WHEN delay > 0 THEN 'Tolerable Delay' \n",
    "WHEN delay = 0 THEN 'No Delay' \n",
    "ELSE 'Early' \n",
    "END AS Flight_Delays\n",
    "FROM us_delay_flights_tbl \n",
    "ORDER BY origin, delay DESC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SQL Databases and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### // Creating a managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS managed_us_delay_flights_tb_3 \n",
    "(date STRING, delay INT,distance INT, origin STRING, destination STRING)USING csv\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### // Creating a unmanaged table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, distance INT, origin STRING, destination STRING) \n",
    "USING csv OPTIONS (PATH'./departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view \n",
    "AS SELECT date, delay, origin, destination \n",
    "from us_delay_flights_tbl WHERE origin = 'SFO';\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    "SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    "origin = 'JFK'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve created these views, you can issue queries against them just as you would against a table. Keep in mind that when accessing a global temporary view you must use the prefix global_temp., because Spark creates global temporary views in a global temporary database called global_temp. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.sql(\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\")\n",
    "// By contrast, you can access the normal temporary view without the global_temp\n",
    "// prefix:\n",
    "\n",
    "\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")\n",
    "\n",
    "\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\")\n",
    "\n",
    "// Or\n",
    "\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")\n",
    "// You can also drop a view just like you would a table:\n",
    "\n",
    "spark.sql(\"DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view;\")\n",
    "spark.sql(\"DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view\")\n",
    "// In Scala/Python\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sources for DataFrames and SQL Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n",
       "df2: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n",
       "df3: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n",
       "df4: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\"\n",
    "\n",
    "// Use Parquet\n",
    "val df = spark.read.format(\"parquet\").load(file)\n",
    "\n",
    "// Use Parquet; you can omit format(\"parquet\") if you wish as it's the default\n",
    "val df2 = spark.read.load(file)\n",
    "\n",
    "// Use CSV\n",
    "val df3 = spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"mode\", \"PERMISSIVE\")\n",
    ".load(\"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\")\n",
    "\n",
    "// Use JSON\n",
    "val df4 = spark.read.format(\"json\").load(\"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"compression\", \"snappy\")\n",
    ".save(\"./df_parquet_flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\n",
    ".mode(\"overwrite\")\n",
    ".saveAsTable(\"us_delay_flights_tbl_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrames to JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"json\")\n",
    ".mode(\"overwrite\")\n",
    "//.option(\"compression\", \"snappy\")\n",
    ".save(\"./df_json_sc2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "USING csv\n",
    "OPTIONS (\n",
    "path \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\",\n",
    "header \"true\",\n",
    "inferSchema \"true\",\n",
    "mode \"FAILFAST\"\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### writing csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"./df_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME               |ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------------------+-------------------+-----+\n",
      "|United States                   |Romania            |1    |\n",
      "|United States                   |Ireland            |264  |\n",
      "|United States                   |India              |69   |\n",
      "|Egypt                           |United States      |24   |\n",
      "|Equatorial Guinea               |United States      |1    |\n",
      "|United States                   |Singapore          |25   |\n",
      "|United States                   |Grenada            |54   |\n",
      "|Costa Rica                      |United States      |477  |\n",
      "|Senegal                         |United States      |29   |\n",
      "|United States                   |Marshall Islands   |44   |\n",
      "|Guyana                          |United States      |17   |\n",
      "|United States                   |Sint Maarten       |53   |\n",
      "|Malta                           |United States      |1    |\n",
      "|Bolivia                         |United States      |46   |\n",
      "|Anguilla                        |United States      |21   |\n",
      "|Turks and Caicos Islands        |United States      |136  |\n",
      "|United States                   |Afghanistan        |2    |\n",
      "|Saint Vincent and the Grenadines|United States      |1    |\n",
      "|Italy                           |United States      |390  |\n",
      "|United States                   |Russia             |156  |\n",
      "+--------------------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"avro\")\n",
    ".load(\"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\")\n",
    "\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW episode_tbl\n",
    "USING avro\n",
    "OPTIONS (\n",
    "path \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\n",
    ".format(\"avro\")\n",
    ".mode(\"overwrite\")\n",
    ".save(\"./df_avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"C:/Users/jorgedario.mendez/OneDrive - Bosonit/Documentos/3. Libro/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "val df = spark.read.format(\"orc\").load(file)\n",
    "df.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"orc\")\n",
    ".mode(\"overwrite\")\n",
    ".option(\"compression\", \"snappy\")\n",
    ".save(\"./df_orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.source.image\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.source.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "imageDir: String = ./cctvVideos/train_images/\r\n",
       "imagesDF: org.apache.spark.sql.DataFrame = [image: struct<origin: string, height: int ... 4 more fields>, label: int]\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val imageDir = \"./cctvVideos/train_images/\"\n",
    "val imagesDF = spark.read.format(\"image\").load(imageDir)\n",
    "imagesDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+----+-----+\n",
      "|height|width|nChannels|mode|label|\n",
      "+------+-----+---------+----+-----+\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |1    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "+------+-----+---------+----+-----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "imagesDF.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\"label\").show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "|                path|   modificationTime|length|             content|label|\n",
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 55037|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54634|[FF D8 FF E0 00 1...|    1|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54624|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54505|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54475|[FF D8 FF E0 00 1...|    0|\n",
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path: String = ./cctvVideos/train_images/\r\n",
       "binaryFilesDF: org.apache.spark.sql.DataFrame = [path: string, modificationTime: timestamp ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"./cctvVideos/train_images/\"\n",
    "val binaryFilesDF = spark.read.format(\"binaryFile\")\n",
    ".option(\"pathGlobFilter\", \"*.jpg\")\n",
    ".load(path)\n",
    "binaryFilesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 55037|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54634|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54624|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54505|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/jo...|2021-04-15 02:34:17| 54475|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "binaryFilesDF: org.apache.spark.sql.DataFrame = [path: string, modificationTime: timestamp ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val binaryFilesDF = spark.read.format(\"binaryFile\")\n",
    ".option(\"pathGlobFilter\", \"*.jpg\")\n",
    ".option(\"recursiveFileLookup\", \"true\")\n",
    ".load(path)\n",
    "binaryFilesDF.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
