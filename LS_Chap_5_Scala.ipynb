{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Spark - Chapter 5 (Scala)\n",
    "------------------------------------\n",
    "## Spark SQL and DataFrames: Interacting with External Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002778.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1620562319893)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@77b71e02\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(\"UDF\")\n",
    "//.config(\"spark.jars.packages\", \"com.databricks:spark-avro_2.12:3.1.1\")\n",
    "//.config(\"spark.sql.catalogImplementation\",\"hive\")\n",
    ".enableHiveSupport()\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cubed: Long => Long = $Lambda$1992/0x0000000801541728@36ef0b90\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create cubed function\n",
    "val cubed = (s: Long) => {\n",
    "    s * s * s\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$1992/0x0000000801541728@36ef0b90,LongType,List(Some(class[value[0]: bigint])),Some(class[value[0]: bigint]),Some(cubed),false,true)\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Register UDF\n",
    "spark.udf.register(\"cubed\", cubed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jdbcDF1: org.apache.spark.sql.DataFrame = [customerid: string, firstname: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read Option 1: Loading data from a JDBC source using load method\n",
    "val jdbcDF1 = spark\n",
    ".read\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", \"jdbc:postgresql://localhost:5432/Clothe Store\")\n",
    ".option(\"dbtable\", \"public.customers\")\n",
    ".option(\"user\", \"postgres\")\n",
    ".option(\"password\", \"Bosonit2021!\")\n",
    ".option(\"driver\", \"org.postgresql.Driver\")\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Properties\r\n",
       "cxnProp: java.util.Properties = {password=Bosonit2021!, user=postgres}\r\n",
       "res3: Object = null\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read Option 2: Loading data from a JDBC source using jdbc method\n",
    "\n",
    "// Create connection properties\n",
    "import java.util.Properties\n",
    "val cxnProp = new Properties()\n",
    "cxnProp.put(\"user\", \"postgres\")\n",
    "cxnProp.put(\"password\", \"Bosonit2021!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jdbcDF2: org.apache.spark.sql.DataFrame = [customerid: string, firstname: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load data using the connection properties\n",
    "val jdbcDF2 = spark\n",
    ".read\n",
    ".option(\"driver\", \"org.postgresql.Driver\")\n",
    ".jdbc(\"jdbc:postgresql://localhost:5432/Clothe Store\", \"public.customers\", cxnProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF2.createOrReplaceTempView(\"tabla1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+--------------+----------------+\n",
      "|customerid|firstname|  surname|shipping_state|loyalty_discount|\n",
      "+----------+---------+---------+--------------+----------------+\n",
      "| 200000903|     Jake|   Peters|       Georgia|             0.1|\n",
      "| 100000906| Caroline|Robertson|       Illinoi|             0.0|\n",
      "| 200000258|     Owen|  McGrath|      Arkansas|            0.08|\n",
      "| 100000937|    Karen|    White|     Tennessee|            0.09|\n",
      "| 100000890|    Piers|    Peake|   Connecticut|             0.0|\n",
      "| 200000460|   Olivia|   Turner|          Ohio|            0.02|\n",
      "| 100000169|    Blake|    Mills|      Kentucky|            0.09|\n",
      "| 200000388|    Gavin|Sanderson|       Georgia|            0.03|\n",
      "| 200000532|    Frank|     Parr|       Alabama|             0.0|\n",
      "| 200000263|Gabrielle| Marshall|      Michigan|            0.07|\n",
      "| 100000884|     Lisa|   Turner|    New Mexico|             0.1|\n",
      "| 400000541| Victoria|     Kerr|      Arkansas|            0.01|\n",
      "| 100000853|   Claire|   Hudson|          Iowa|            0.03|\n",
      "| 100000831|   Steven| Morrison|  South Dakota|             0.1|\n",
      "| 100000694|  Michael|  Cameron|       Alabama|            0.04|\n",
      "| 100000559|    Grace|    Mills| Massachusetts|            0.09|\n",
      "| 100000255|    Diane|Henderson|        Kansas|             0.1|\n",
      "| 100000296| Penelope| Clarkson|          Ohio|            0.07|\n",
      "| 100000616|   Alison|  Cornish|       Alabama|             0.1|\n",
      "| 300000358|    Simon| Morrison|      Colorado|            0.04|\n",
      "+----------+---------+---------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM tabla1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|slice(array(1, 2, 3, 4), -3, 2)|\n",
      "+-------------------------------+\n",
      "|                         [2, 3]|\n",
      "+-------------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT slice(array(1, 2, 3,\n",
    "4), -3, 2)\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher-Order Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t1: Array[Int] = Array(35, 36, 32, 30, 40, 42, 38)\r\n",
       "t2: Array[Int] = Array(31, 32, 34, 55, 56)\r\n",
       "tC: org.apache.spark.sql.DataFrame = [celsius: array<int>]\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create DataFrame with two rows of two arrays (tempc1, tempc2)\n",
    "val t1 = Array(35, 36, 32, 30, 40, 42, 38)\n",
    "val t2 = Array(31, 32, 34, 55, 56)\n",
    "val tC = Seq(t1, t2).toDF(\"celsius\")\n",
    "\n",
    "tC.createOrReplaceTempView(\"tC\")\n",
    "\n",
    "// Show the DataFrame\n",
    "tC.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Calculate Fahrenheit from Celsius for an array of temperatures\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\n",
    "FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "filter(celsius, t -> t > 38) as high\n",
    "FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Is there a temperature of 38C in the array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "exists(celsius, t -> t = 38) as threshold\n",
    "FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 2 pos 0\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 2 pos 0\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$16.$anonfun$applyOrElse$121(Analyzer.scala:2068)\r",
      "  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$16.applyOrElse(Analyzer.scala:2068)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$16.applyOrElse(Analyzer.scala:2059)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:317)\r",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:317)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:322)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:322)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDown$1(QueryPlan.scala:94)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)\r",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r",
      "  at scala.collection.immutable.List.foreach(List.scala:392)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r",
      "  at scala.collection.immutable.List.map(List.scala:298)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:94)\r",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:85)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressions$1.applyOrElse(AnalysisHelper.scala:151)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressions$1.applyOrElse(AnalysisHelper.scala:150)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)\r",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressions(AnalysisHelper.scala:150)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressions$(AnalysisHelper.scala:149)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressions(LogicalPlan.scala:29)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2059)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2056)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\r",
      "  at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\r",
      "  at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\r",
      "  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\r",
      "  at scala.collection.immutable.List.foreach(List.scala:392)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\r",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\r",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\r",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\r",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\r",
      "  ... 38 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "// Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "reduce(celsius,0,(t, acc) -> t + acc,\n",
    "acc -> (acc div size(celsius) * 9 div 5) + 32) as avgFahrenheit FROM tC\"\"\")\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common DataFrames and Spark SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airportsPath: String = ./airport-codes-na.txt\r\n",
       "delaysPath: String = ./departuredelays.csv\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Set file paths\n",
    "val airportsPath = \"./airport-codes-na.txt\"\n",
    "val delaysPath = \"./departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airports: org.apache.spark.sql.DataFrame = [City: string, State: string ... 2 more fields]\r\n",
       "delays: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Obtain airports data set\n",
    "val airports = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferschema\", \"true\")\n",
    ".option(\"delimiter\", \"\\t\")\n",
    ".csv(airportsPath)\n",
    "\n",
    "airports.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "// Obtain departure Delays data set\n",
    "val delays = spark.read\n",
    ".option(\"header\",\"true\")\n",
    ".csv(delaysPath)\n",
    ".withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    ".withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\"))\n",
    "\n",
    "delays.createOrReplaceTempView(\"departureDelays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create temporary small table\n",
    "val foo = delays.filter(\n",
    "expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND\n",
    "date like '01010%' AND delay > 0\"\"\"))\n",
    "\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bar: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Union two tables\n",
    "val bar = delays.union(foo)\n",
    "\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM bar\n",
    "WHERE origin = 'SEA'\n",
    "AND destination = 'SFO'\n",
    "AND date LIKE '01010%'\n",
    "AND delay > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "foo.join(airports.as(\"air\"), $\"air.IATA\" === $\"origin\")\n",
    ".select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\")\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination\n",
    "FROM foo f\n",
    "JOIN airports_na a\n",
    "ON a.IATA = f.origin\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"DROP TABLE IF EXISTS departureDelaysWindow;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|origin|destination|TotalDelays|\n",
      "+------+-----------+-----------+\n",
      "|   JFK|        ORD|       5608|\n",
      "|   SEA|        LAX|       9359|\n",
      "|   JFK|        SFO|      35619|\n",
      "|   SFO|        ORD|      27412|\n",
      "|   JFK|        DEN|       4315|\n",
      "|   SFO|        DEN|      18688|\n",
      "|   SFO|        SEA|      17080|\n",
      "|   SEA|        SFO|      22293|\n",
      "|   JFK|        ATL|      12141|\n",
      "|   SFO|        ATL|       5091|\n",
      "|   SEA|        DEN|      13645|\n",
      "|   SEA|        ATL|       4535|\n",
      "|   SEA|        ORD|      10041|\n",
      "|   JFK|        SEA|       7856|\n",
      "|   JFK|        LAX|      35755|\n",
      "|   SFO|        JFK|      24100|\n",
      "|   SFO|        LAX|      40798|\n",
      "|   SEA|        JFK|       4667|\n",
      "+------+-----------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT origin, destination, SUM(delay) AS TotalDelays\n",
    "FROM departureDelays\n",
    "WHERE origin IN ('SEA', 'SFO', 'JFK')\n",
    "AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')\n",
    "GROUP BY origin, destination;\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Hive support is required to CREATE Hive TABLE (AS SELECT);\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);\r",
      "'CreateTable `default`.`departureDelaysWindow`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\r",
      "+- Aggregate [origin#167, destination#168], [origin#167, destination#168, sum(cast(delay#175 as bigint)) AS TotalDelays#502L]\r",
      "   +- Filter (origin#167 IN (SEA,SFO,JFK) AND destination#168 IN (SEA,SFO,JFK,DEN,ORD,LAX,ATL))\r",
      "      +- SubqueryAlias departuredelays\r",
      "         +- Project [date#164, delay#175, cast(distance#166 as int) AS distance#182, origin#167, destination#168]\r",
      "            +- Project [date#164, cast(delay#165 as int) AS delay#175, distance#166, origin#167, destination#168]\r",
      "               +- Relation[date#164,delay#165,distance#166,origin#167,destination#168] csv\r",
      "\r",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.$anonfun$apply$4(rules.scala:462)\r",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.$anonfun$apply$4$adapted(rules.scala:460)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:173)\r",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:460)\r",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:458)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$46(CheckAnalysis.scala:699)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$46$adapted(CheckAnalysis.scala:699)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:699)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\r",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\r",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\r",
      "  ... 40 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE departureDelaysWindow AS\n",
    "SELECT origin, destination, SUM(delay) AS TotalDelays\n",
    "FROM departureDelays\n",
    "WHERE origin IN ('SEA', 'SFO', 'JFK')\n",
    "AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')\n",
    "GROUP BY origin, destination;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Table or view not found: departureDelaysWindow; line 1 pos 14;\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Table or view not found: departureDelaysWindow; line 1 pos 14;\r",
      "'Project [*]\r",
      "+- 'UnresolvedRelation [departureDelaysWindow], [], false\r",
      "\r",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:113)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:182)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:182)\r",
      "  at scala.collection.immutable.List.foreach(List.scala:392)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:182)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)\r",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\r",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\r",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\r",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\r",
      "  ... 40 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM departureDelaysWindow\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\r\n",
       "foo2: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Adding new columns\n",
    "// To add a new column to the foo DataFrame, use the withColumn() method:\n",
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "val foo2 = foo.withColumn(\n",
    "    \"status\",\n",
    "    expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    ")\n",
    "\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "foo3: org.apache.spark.sql.DataFrame = [date: string, distance: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Dropping columns\n",
    "// To drop a column, use the drop() method. For example, let’s remove the delay column\n",
    "// as we now have a status column, added in the previous section:\n",
    "\n",
    "val foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "foo4: org.apache.spark.sql.DataFrame = [date: string, distance: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Renaming columns\n",
    "// You can rename a column using the rename() method:\n",
    "\n",
    "val foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|destination|month|delay|\n",
      "+-----------+-----+-----+\n",
      "|        ORD|    1|   92|\n",
      "|        JFK|    1|   -7|\n",
      "|        DFW|    1|   -5|\n",
      "|        MIA|    1|   -3|\n",
      "|        DFW|    1|   -3|\n",
      "|        DFW|    1|    1|\n",
      "|        ORD|    1|  -10|\n",
      "|        DFW|    1|   -6|\n",
      "|        DFW|    1|   -2|\n",
      "|        ORD|    1|   -3|\n",
      "|        ORD|    1|    0|\n",
      "|        DFW|    1|   23|\n",
      "|        DFW|    1|   36|\n",
      "|        ORD|    1|  298|\n",
      "|        JFK|    1|    4|\n",
      "|        DFW|    1|    0|\n",
      "|        MIA|    1|    2|\n",
      "|        DFW|    1|    0|\n",
      "|        DFW|    1|    0|\n",
      "|        ORD|    1|   83|\n",
      "+-----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// Pivoting\n",
    "// When working with your data, sometimes you will need to swap the columns for the\n",
    "// rows—i.e., pivot your data:\n",
    "\n",
    "spark.sql(\"\"\"SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "FROM departureDelays\n",
    "WHERE origin = 'SEA'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "FROM departureDelays WHERE origin = 'SEA'\n",
    ")\n",
    "PIVOT (\n",
    "CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
    "FOR month IN (1 JAN, 2 FEB)\n",
    ")\n",
    "ORDER BY destination\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
